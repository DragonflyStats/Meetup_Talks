Introduction
Let's begin our discussion on robust regression with some terms in linear regression.


* [Residual] The difference between the predicted value (based on the regression equation) and the actual, observed value.

* [Outlier] In linear regression, an outlier is an observation with  residual. In other words, it is an observation whose dependent-variable value is unusual given its value on the predictor variables. An outlier may indicate a sample peculiarity or may indicate a data entry error or other problem.

* [Leverage] An observation with an extreme value on a predictor variable is a point with high leverage. Leverage is a measure of how far an independent variable deviates from its mean. High leverage points can have a great amount of effect on the estimate of regression coefficients.

* [Influence] An observation is said to be influential if removing the observation substantially changes the estimate of the regression coefficients.  Influence can be thought of as the product of leverage and outlierness.



Cook's distance (or Cook's D): A measure that combines the information of leverage and residual of the observation.


% - http://stats.stackexchange.com/questions/65912/precise-meaning-of-and-comparison-between-influential-point-high-leverage-point





------------------------------------------------------

### Some Important Definitions}

%
%###More Definitions}
To understand a diagnostic plot called the residual-leverage plot, we must understand three things:


*   Leverage,
*   Standardized residuals, and
*   Cook's distance.


%Move next bit to "Leverarge"

% Standardization

% Cook's Distance
%#### Cook's Distance}
%One way to think about whether or not the results you have were driven by a given data point is to calculate how far the predicted values for your data would move if your model were fit without the data point in question. 

%This calculated total distance is called \textbf{Cook's distance}. Fortunately, you don't have to rerun your regression model N times to find out how far the predicted values will move, Cook's D is a function of the leverage and standardized residual associated with each data point.

\###*{Example}

Consider the plots associated with four different situations:

*   a dataset where everything is fine
*   a dataset with a high-leverage, but low-standardized residual point
*   a dataset with a low-leverage, but high-standardized residual point
*   a dataset with a high-leverage, high-standardized residual point-----------------------------------------

\begin{figure}[h!]
\centering
\includegraphics[width=1.0\linewidth]{plots2}
\caption{}
\label{fig:plots2}
\end{figure}

\begin{figure}[h!]
\centering
\includegraphics[width=1.0\linewidth]{plots3}
\caption{}
\label{fig:plots3}
\end{figure}


* The plots on the left show the data, the center of the data  with a blue dot, the underlying data generating process with a dashed gray line, the model fit with a blue line, and the special point with a red dot. 
* On the right are the corresponding residual-leverage plots; the special point is 21. 
* The model is badly distorted primarily in the fourth case where there is a point with high leverage and a  (negative) standardized residual. 



For reference, here are the values associated with the special points:
```{r}
                               leverage std.residual   cooks.d
high leverage,  low residual  0.3814234    0.0014559 0.0000007
low leverage,  high residual  0.0476191    3.4456341 0.2968102
high leverage, high residual  0.3814234   -3.8086475 4.4722437
```


--------------------------------------------------- % 
### Leverage and Influence}

###Summary of Influence Statistics}
###rstudent}
The studentized residual RSTUDENT is estimated by $s(1. ^2$ without the ith observation, not by $s^2$. For example,

\[\mbox{RSTUDENT()</tt>= \frac{r_i}{s_{(1. ()</tt>\sqrt{(1 - h_1. }()</tt>\]
Observations with RSTUDENT r than 2 in absolute value may need some attention.






*  \textbf{Studentized Residuals()</tt>â Residuals divided by their estimated standard errors (like t-statistics). Observations with values r than 3 in absolute value are considered outliers.
*  \textbf{Leverage Values (Hat Diag)()</tt>â Measure of how far an observation is from the others in terms of the levels of the independent variables (not the dependent variable). Observations with values r than $2(k+1)/n$ are considered to be potentially highly influential, where k is the number of predictors and n is the sample size.
*  \textbf{DFFITS()</tt>â Measure of how much an observation has effected its fitted value from the regression model. Values r than $2\sqrt{(k+1)/n}$ in absolute value are considered highly influential. %Use standardized DFFITS in SPSS.
*  \textbf{DFBETAS()</tt>â Measure of how much an observation has effected the estimate of a regression coefficient (there is one DFBETA for each regression coefficient, including the intercept). Values r than 2/sqrt(n) in absolute value are considered highly influential.
\\
The measure that measures how much impact each observation has on a particular predictor is DFBETAs The DFBETA for a predictor and for a particular observation is the difference between the regression coefficient calculated for all of the data and the regression coefficient calculated with the observation deleted, scaled by the standard error calculated with the observation deleted. 

*  \textbf{Cookâs D()</tt>â Measure of aggregate impact of each observation on the group of regression coefficients, as well as the group of fitted values. Values r than 4/n are considered highly influential.



### Other Measures of Influence}
The impact of an observation on a regression fitting can be determined by the difference between the estimated regression coefficient of a model with all observations and the estimated coefficient when the particular observation is deleted. The measure DFBETA is the studentized value of this difference.

Influence arises at two stages of the LME model. Firstly when $V$ is estimated by $\hat{V}$, and subsequent
estimations of the fixed and random regression coefficients $\beta$ and $u$, given $\hat{V}$.



#### DFBETA}
A group of measures that measures how much impact each observation has on a particular predictor are the DFBETAs. The DFBETA for a predictor and for a particular observation is the difference between the regression coefficient calculated for all of the data and the regression coefficient calculated with the observation deleted,  scaled by the standard error calculated with the observation deleted.

%#### DFBETA()</tt>%1.16.3
\begin{eqnarray}
DFBETA_{a()</tt>&=& \hat{\beta()</tt>- \hat{\beta}_{(a)()</tt>\\
&=& B(Y-Y_{\bar{a}}
\end{eqnarray}

For $k$ predictors in the mode, there ar $k+1$ dfbetas.

% Interpret


#### DFFITS()</tt>%1.16.1
DFFITS is a statistical measured designed to a show how influential an observation is in a statistical model. It is closely related to the studentized residual.
\begin{displaymath()</tt>DFFITS = {\widehat{y_i()</tt>-
\widehat{y_{i(k)}()</tt>\over s_{(k)()</tt>\sqrt{h_{ii}}()</tt>\end{displaymath}


#### COVRATIO}
The COVRATIO statistic measures the change in 
the determinant of the covariance matrix of the estimates by deleting the ith observation:

\[ COVRATIO = \frac{det ( s^2_{(1. ()</tt>(X_{(1. }'X_{(1. })^{-1()</tt>) )}{ det ( s^2 (X'X)^{-1()</tt>) ()</tt> \]
%Belsley, Kuh, and Welsch suggest that observations with

Observations with

\[|{COVRATIO()</tt>- 1| \ge \frac{3k}{n}\]
where k is the number of parameters in the model and n is the number of observations used to fit the model, are worth further investigation.

%#### PRESS()</tt>%1.16.2
%The prediction residual sum of squares (PRESS) is an value associated with this calculation. When fitting linear models, PRESS can be used as a criterion for model selection, with smaller values indicating better model fits.
%\begin{equation}
%PRESS = \sum(y-y^{(k)})^2
%\end{equation}
%
%
%
%*   $e_{-Q()</tt>= y_{Q()</tt>- x_{Q}\hat{\beta}^{-Q}$
%*   $PRESS_{(U)()</tt>= y_{i()</tt>- x\hat{\beta}_{(U)}$
%


------------------------------------------------------------ %


###Influential Observations : DFBeta and DFBetas}
%% http://stats.stackexchange.com/questions/22161/how-to-read-cooks-distance-plots
%Cook's distance refers to how far, on average, predicted y-values will move if the observation in question is dropped from the data set. dfbeta refers to how much a parameter estimate changes if the observation in question is dropped from the data set. Note that with k covariates, there will be k+1 dfbetas (the intercept,$\beta_0$, and 1 $\beta$ for each covariate). Cook's distance is presumably more important to you if you are doing predictive modeling, whereas dfbeta is more important in explanatory modeling.




