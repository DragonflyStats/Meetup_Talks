\documentclass[a4paper,12pt]{article}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\usepackage{eurosym}
\usepackage{vmargin}
\usepackage{amsmath}
\usepackage{graphics}
\usepackage{epsfig}
\usepackage{subfigure}
\usepackage{enumerate}
\usepackage{fancyhdr}

\setcounter{MaxMatrixCols}{10}
%TCIDATA{OutputFilter=LATEX.DLL}
%TCIDATA{Version=5.00.0.2570}
%TCIDATA{<META NAME="SaveForMode"CONTENT="1">}
%TCIDATA{LastRevised=Wednesday, February 23, 201113:24:34}
%TCIDATA{<META NAME="GraphicsSave" CONTENT="32">}
%TCIDATA{Language=American English}

\pagestyle{fancy}
\setmarginsrb{20mm}{0mm}{20mm}{25mm}{12mm}{11mm}{0mm}{11mm}
\lhead{MA4128} \rhead{Kevin O'Brien} \chead{Principal Component Analysis} %\input{tcilatex}

\begin{document}

\begin{itemize}
\item Principal Component Analysis (PCA) is a mathematical technique for dimension reduction of a dataset containing $p$ variables, where $p$ is a considered a large number. \textit{This is a judgement call.}
\item \textbf{Important:} PCA transforms the initial features (i.e. variables) into new ones, that are linear combinations of the original features. 

\item \textbf{Important:} You can then select $k$ features among a larger set of $p$ features, with $k$ much smaller than $p$.

\item This smaller set of $k$ features built with PCA is the best subset of $k$ features, in the sense that it minimizes the variance of the residual noise when fitting data to a linear model. 
\end{itemize}
\subsection*{Steps for PCA}

The PCA algorithm proceeds as follows:
\begin{enumerate}
    \item Normalize the original features: remove the mean from each  feature
\item Compute the covariance matrix on the normalized data. This is an 
$n \times  n$ symmetric matrix, where n is the number of original features, and the element in row i and column j is the covariance between the $i-$th and $j-$th column in the data set.  
\item Calculate the eigenvectors and eigenvalues of the covariance matrix. These eigenvectors must be unit eigenvectors, that is, their lengths are 1. This step is the most intricate, and most software packages can do it automatically.
\item Choose the $k$ eigenvectors with the highest eigenvalues.
\item Compute the final $k$ features, associated with the $k$ highest eigenvalues: for each one, multiply the data set matrix, by the associated eigenvector.

 Here we assume that the eigenvector has one column and n rows (n is the number of original variables), while the data set matrix has n columns and m rows (m is the number of observations), Thus the resulting final features have m rows and one column: it provides the values for the new features, computed at each of the m observations.
\item You may want to put back the mean that was removed in step 1.
\end{enumerate}
The proportion of the variance that each eigenvector represents can be calculated by dividing the eigenvalue corresponding to that eigenvector by the sum of all eigenvalues.

\subsection*{Caveats}

If the original features are highly correlated, the solution will be very unstable. Also the new features are linear combinations of the original features, and thus, may lack interpretation. The data does not need to be multinormal, except if you use this technique for predictive modeling using normal models to compute confidence intervals.

\end{document}
