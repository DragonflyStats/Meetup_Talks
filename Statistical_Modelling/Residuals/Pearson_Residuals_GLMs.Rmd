---
title: "Pearson Residuals"
subtitle: "General Linear Modesl"
author: DragonflyStats.github.io
output:
  prettydoc::html_pretty:
    theme: cayman
    highlight: github
---


### Pearson Residuals






The Pearson residual is the raw residual divided by the square root of the variance function $V(\mu).$
The Pearson residual is the individual contribution to the Pearson $\chi^2$ statistic. 

For a binomial distribution with mi trials in the ith observation, it is defined as

\[ r_{Pi} = \sqrt{ m_{i}}
 \frac{r_{i}}{\sqrt{V(\hat{ \mu_{i}})}} \]
For other distributions, the Pearson residual is defined as

\[ r_{Pi} = \frac{r_{i}}{\sqrt{V(\hat{ \mu_{i}})}}\]
The Pearson residuals can be used to check the model fit at each observation for generalized linear models. 
%These residuals are stored in variables named RP_yname for each response variable, where yname is the response variable name. 
The standardized and studentized Pearson residuals are
\[
r_{Psi} = \frac{r_{Pi}}{\sqrt{\hat{ \phi} (1- h_{i})} } \]
\[ r_{Pti} = \frac{r_{Pi}}{\sqrt{ \hat{ \phi}_{(i)}
 (1- h_{i})} } \]




```{r, include=FALSE,echo=FALSE}


### Pearson and Deviance Residuals

# https://v8doc.sas.com/sashtml/insight/chap39/sect55.htm


# - http://stats.stackexchange.com/questions/24975/logistic-regression-getting-pearson-standardized-residuals-in-r-vs-stata
# - https://onlinecourses.science.psu.edu/stat504/book/export/html/160
```

* Pearson residuals are obtained by dividing the each observation's raw residual by the square root of the corresponding variance. 

* The idea is to get something that has variance 1, approximately. In your example, try this;


```{r}
set.seed(3141)
x1 <- rnorm(100)
x2 <- rnorm(100)
y <- rbinom(100, 1, 0.25)
```


```{r}
glm1 <- glm(y~x1+x2, family=binomial)
f1 <- fitted(glm1) # the fitted probability of y=1, for each observation
```

```{r}
plot( residuals(glm1, "pearson"), (y-f1)/sqrt(f1*(1-f1)))
abline(0,1)        # they match

```


The 'gap' occurs because the residuals where Y=1 are on one side, and those with Y=0 are on the other. 

Standardized residuals are a different animal; they divide by the estimated standard deviation of the residual; you can obtain them in R using <tt>rstandard()</tt>, though for non-linear GLMs it uses a linear approximation in the calculation.

NB residuals of any form tend not to be terribly helpful in logistic regression. 

With independent binary data, the only real concern is whether we've specified the mean correctly - and with modest sample sizes, plots of residuals typically provide little power to assess that.

It looks like you're sharing a detailed document about Pearson Residuals in General Linear Models. This is a great resource for understanding how Pearson residuals are calculated and used in the context of generalized linear models, especially for checking model fit.

Here's a brief summary of the key points:

### Pearson Residuals:
- **Definition:** The raw residual divided by the square root of the variance function \(V(\mu)\).
- **Purpose:** Individual contribution to the Pearson \(\chi^2\) statistic.
- **Calculation for Binomial Distribution:**
  \[
  r_{Pi} = \sqrt{ m_{i}} \frac{r_{i}}{\sqrt{V(\hat{ \mu_{i}})}}
  \]
- **Calculation for Other Distributions:**
  \[
  r_{Pi} = \frac{r_{i}}{\sqrt{V(\hat{ \mu_{i}})}}
  \]
- **Usage:** Checking model fit at each observation for generalized linear models.
- **Standardized and Studentized Pearson Residuals:**
  \[
  r_{Psi} = \frac{r_{Pi}}{\sqrt{\hat{ \phi} (1- h_{i})} } \quad \text{and} \quad r_{Pti} = \frac{r_{Pi}}{\sqrt{ \hat{ \phi}_{(i)} (1- h_{i})} }
  \]

The document also mentions the practical implementation and plotting of Pearson residuals in R, emphasizing the importance of checking model fit and variance. If you have any specific questions or need further explanation on any part of the document, feel free to ask!

