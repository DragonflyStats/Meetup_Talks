
### Akaike's Information Criterion} %\input{tcilatex}

\section*{R Square}
The model with the highest $R^2$ and adjusted $R^2$  is the preferable of all candidate models
The quadratic model is the preferable model in that case.


* This coefficient is a measure of how much of the variance in the observed values of the dependent variable (Y) can be explained by it's relationship to the independent variable (X).
* In other words, the coefficient of determination is the percent of the variation explained by the \textbf{\textit{linear regression equation}}.


* The coefficient of determination is also called \textbf{\textit{r-square}} or \textbf{\textit{r-squared}}. 
* It can be computed by simply squaring the (Pearson) correlation coefficient value (\textbf{r}).
* \textit{Strictly speaking this is not the actual definition of r-squared. The coefficent of determination is the ratio of two sums of squares identities.}


* $R^2$ is a measure of variation explained by regression.

* The following coefficient has a natural interpretation as amount
of variability in the data that is explained by the regression
fit: $R^2 = SSLR/SST = 1 - SSR/SST$.

* A similar interpretation is given to the adjusted coefficient
$R^2_{adj}$ which is given by $R^2_{adj} = 1 - MSR/MST $; where
MSR is the mean squared error due to residuals, and MST is the
total mean squared error.




%%----------------------------------------------------%%
\section{Adjusted R Square}


* Model selection is the task of selecting a statistical model from a set of potential models, given data.
* In a multiple linear regression model, adjusted R square measures the proportion of the variation in the dependent variable 
accounted for by the independent variables.  
* Adjusted R square is generally considered to be a more accurate goodness-of-fit measure than R square.
* The adjusted coefficient is accounting for the degrees of freedom
used for each source of variation and is often a more reliable
indicator of variability than $R^2$. $R^2_{adj}$ is always smaller
than $R^2$.
* $R^2$ is a measure of variation explained by regression.



\subsection*{The Coefficient of Determination}

* The coefficient of determination $R^2$ is the proportion of variability in a data set that is accounted for by the linear model. 
* Equivalently $R^2$ provides a measure of how well future outcomes are likely to be predicted by the model.

* For simple linear regression, it can be computed by squaring the correlation coefficient. It is not specifically defined that way. 
* This relationship is co-incidental when there are just two variables.






{

\begin{framed}
\begin{verbatim}
> summary(lm(Y~X))

Call:
lm(formula = Y ~ X)
....

Coefficients:
Estimate Std. Error t value
(Intercept) -18.5506    41.4156  -0.448
X             1.1855     0.4073   2.911
Pr(>|t|)  
(Intercept)   0.6661  
X             0.0196 *
....
Residual standard error: 3.884 on 8 degrees of freedom
Multiple R-squared:  0.5143,    Adjusted R-squared:  0.4536 
F-statistic: 8.472 on 1 and 8 DF,  p-value: 0.01957

\end{verbatim}
\end{framed}
}



%%----------------------------------------------------%%
\section{Adjusted R square}

\noindent The adjusted R-square value is found on the summary output for a fitted model. 
It is called \textbf{\emph{adjusted}} because it takes into account the number of predictor variables being used. 
The law of parsimony states the simplest model that adequately explains the outcomes is the best. 
The candidate model with the higher adjusted R squared is considered preferable.

Information Criterions
We define two types of information criterion: the Akaike Information Criterion (AIC) and the
Schwarz?s Bayesian Information Criterion (BIC). The Akaike information criterion is a measure
of the relative goodness of fit of a statistical model.
AIC = 2p ? 2 ln(L)


%=================================================%

	* p is the number of predictor variables in the model.
	* L is the value of the Likelihood function for the model in question.
	* For AIC to be optimal, n must be large compared to p.


An alternative to the AIC is the Schwarz BIC, which additionally takes into account the
sample size n.
BIC = p ln n ? 2 ln(L)
When using the AIC (or BIC) for selecting the optimal model, we choose the model for which
the AIC (or BIC) value is lowest.


Akaike Information Criterion


	* Akaike?s information criterionis a measure of the goodness of fit of an estimated statistical
	model. The AIC was developed by Hirotsugu Akaike under the name of ?an information
	criterion? in 1971.
	* The AIC is a model selection tool i.e. a method of comparing two or more candidate
	regression models. The AIC methodology attempts to find the model that best explains
	the data with a minimum of parameters. (i.e. in keeping with the law of parsimony)
	* The AIC is calculated using the ?likelihood function? and the number of parameters .
	The likelihood value is generally given in code output, as a complement to the AIC.
	(Likelihood function is not on our course)
	* Given a data set, several competing models may be ranked according to their AIC, with
	the one having the lowest AIC being the best. (Although, a difference in AIC values of
	less than two is considered negligible).
	



In a multiple linear regression model, adjusted R square measures the proportion of the variation in the dependent variable 
accounted for by the independent variables.  

Adjusted R square is generally considered to be a more accurate goodness-of-fit measure than R square.

