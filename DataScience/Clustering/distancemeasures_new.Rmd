
\pagestyle{fancy}
\setmarginsrb{20mm}{0mm}{20mm}{25mm}{12mm}{11mm}{0mm}{11mm}
\lhead{MA4128} \rhead{Kevin O'Brien} \chead{Distance Measures} %\input{tcilatex}

%http://www.electronics.dit.ie/staff/ysemenova/Opto2/CO_IntroLab.pdf

\noindent \textbf{\textit{(Remark: We will focus on distance measures used in Cluster Analysis here. Later in the semester, we will revert later to other distance metrics, specifically measures that are commonly encountered in multivariate statistics, e.g Mahalanobis Distance.)}}
%http://www.econ.upf.edu/~michael/stanford/maeb4.pdf
%http://stn.spotfire.com/spotfire_client_help/hc/hc_distance_measures_overview.htm
\section*{Distance Measures for Cluster Analysis}

*  There are various measures to express (dis)similarity between pairs of objects.
A straightforward way to assess two objects proximity is by drawing a straight line
between them. This type of distance is also referred to as
\textbf{\textit{Euclidean distance}} (or straight-line distance) and is the most commonly used type
when it comes to analyzing ratio or interval-scaled data.
\begin{figure}[h!]
\begin{center}
% Requires \usepackage{graphicx}
\includegraphics[scale=0.6]{images/EuclidDistance1.jpg}
\end{center}
\end{figure}

The Euclidean distance is the square root of the sum of the squared differences in
the variables values. Suppose B and C were positioned as $(7,6)$ and $(6,5)$ respectively.
\begin{figure}[h!]
\begin{center}
% Requires \usepackage{graphicx}
\includegraphics[scale=0.6]{images/EuclidDistance2.jpg}
\end{center}
\end{figure}

This distance corresponds to the length of the line that connects objects B and C.
In this case, we only used two variables but we can easily add more under the root
sign in the formula. However, each additional variable will add a dimension to our
analysis (e.g., with ten clustering variables, we have to deal with ten
dimensions), making it impossible to represent the solution graphically.

*   The \textbf{\textit{Squared Euclidean distance}} uses the same equation as the Euclidean distance metric, but does not take the square root. For the previous example, the squared Euclidean distance between B and C is 2.
As a result, clustering with the Squared Euclidean distance is computationally faster than clustering with the regular Euclidean distance.

*  We can compute the distance between all other pairs of objects. All
these distances are usually expressed by means of a \textit{\textbf{distance matrix}}. In this distance
matrix, the non-diagonal elements express the distances between pairs of objects
and zeros on the diagonal (the distance from each object to itself is, of course, 0). In
our example, the distance matrix is an $8 \times 8$ table with the lines and rows
representing the objects under consideration.
\begin{figure}[h!]
\begin{center}
% Requires \usepackage{graphicx}
\includegraphics[scale=0.6]{images/DistanceMatrix.jpg}
\end{center}
\end{figure}

*  There are also alternative distance measures: The \textbf{\textit{Manhattan distance}} or city-block distance uses the sum of the variables absolute differences.

* [$\ast$] This is often called the Manhattan metric
as it is akin to the walking distance between two points in a city like New Yorks
Manhattan district, where the distance equals the number of blocks in the directions
North-South and East-West.
  Using the points B and C that we used previously, the manhattan distance is computed as follows:
\begin{figure}[h!]
\begin{center}
% Requires \usepackage{graphicx}
\includegraphics[scale=0.6]{images/Manhattan.jpg}
\end{center}
\end{figure}
*  When working with metric (or ordinal) data, researchers frequently use
the \textbf{\textit{Chebychev distance}}, which is the maximum of the absolute difference in the
clustering variables values. For B and C, this result is:

\begin{figure}[h!]
\begin{center}
% Requires \usepackage{graphicx}
\includegraphics[scale=0.6]{images/Chebyshev.jpg}
\end{center}
\end{figure}
*  There are other distance measures such as the Angular, Canberra or Mahalanobis
distance. In many situations, the \textbf{\textit{Mahalanobis
distance}} is desirable as this measure compensates for \textbf{\textit{multi-collinearity}}
between the clustering variables. However, it is unfortunately not menu-accessible
in SPSS for cluster analysis.

---------------------------------------------------------------------

\section*{Euclidean Distance Metrics}

*  The most straightforward and generally accepted way of computing distances between objects in a multi-dimensional space is to compute Euclidean distances, an extension of Pythagoras's theorem. If we had a two- or three-dimensional space this measure is the actual geometric distance between objects in the space (i.e. as if measured with a ruler).

*  In a univariate example, the Euclidean distance between two values is the arithmetic difference, i.e. \textbf{\textit{value1 - value2}}. In the bivariate case, the minimum distance is the hypotenuse of a triangle formed from the points, as in Pythagoras's theorem.

*  Although difficult to visualize, an extension of the Pythagoras's theorem will give the distance between two points in n-dimensional space. The squared Euclidean distance is used more often than the simple Euclidean distance in order to place progressively greater weight on objects that are further apart. Euclidean (and squared Euclidean) distances are usually computed from raw data, and not from transformed data, e.g. standardized data.




\subsection*{Euclidean Distance}
The Euclidean distance between two points, x and y, with $k$ dimensions is calculated as:
\[ \sqrt{ \sum^{k}_{j=1} ( x_j - y_j)^2 } \]
The Euclidean distance is always greater than or equal to zero. The measurement would be zero for identical points and high for points that show little similarity.

\subsubsection*{Example}
Compute the Euclidean Distance between the following points:
$X = \{1,5,4,3\}$ and $Y = \{2,1,8,7\}$

\begin{center}
\begin{tabular}{|c|c|c|c|}
  \hline
$x_j$&$y_j$&   $x_j - y_j$&$(x_j - y_j)^2$\\ \hline
1&2&-1&1\\
5&1&4&16\\
4&8&-4&16\\
3&7&-4&16\\ \hline
&&&49\\ \hline
\end{tabular}
\end{center}
The Euclidean Distance between the two points is $\sqrt{49}$ i.e. 7.

\subsection*{Standardization}
\noindent The approach to take here is \textbf{standardization}, which is is necessary to balance out the contributions, and the
conventional way to do this is to transform the variables so they all have the same variance
of 1. At the same time we \textbf{\textit{center}} the variables at their means  this centering is not
necessary for calculating distance, but it makes the variables all have mean zero and thus
easier to compare. 

\noindent The transformation commonly called standardization is thus as follows:

\[\mbox{standardized value} = \frac{\mbox{observed value  mean}}{ \mbox{standard deviation}}\]
\begin{center}
\begin{tabular}{|c|c|c|c|c|c|c|}
  \hline
  % after \\: \hline or \cline{col1-col2} \cline{col3-col4} ...
Variables & Case 1 & Case 2 & Mean & Std. Dev & Case 1 (std) & Case 2 (std) \\ \hline
Pollution & 6.0 & 1.9 & 4.517&2.141&0.693&-1.222\\
Depth & 51 & 99 & 74.433&15.615&-1.501&1.573\\
Temp & 3.0 & 2.9 & 3.057&0.281&-0.201&-0.557\\
  \hline
\end{tabular}
\end{center}

\[ d_{std} =  \sqrt{(0.693 - (- 1.222))^2 + (-1.501-1.573)^2 + (-0.201-(-0.557))^2} \]

\[ d_{std} = \sqrt{3.667 + 9.449 + 0.127} = \sqrt{13.243} = 3.639 \]

Pollution and temperature have higher contributions than before but depth still plays the
largest role in this particular example, even after standardization. But this contribution is
justified now, since it does show the biggest standardized difference between the samples. 
--------------------------------------------------------------------------------

\subsection*{Other Measures for Interval Data}
The following dissimilarity measures are available for interval data:

%
%*  Euclidean distance. The square root of the sum of the squared differences between values for the items. This is the default for interval data. {Squared Euclidean distance is the default on the classroom version of SPSS.  This is reasonable given the warning below that SPSS puts in the output.} 
%The squared Euclidean measure should be used when the CENTROID, MEDIAN, or WARD cluster method is requested.
%*  Squared Euclidean distance. The sum of the squared differences between the values for the items. 
*  Pearson correlation. The product-moment correlation between two vectors of values. 
*  Cosine. The cosine of the angle between two vectors of values (see next section). 
*  Chebychev. The maximum absolute difference between the values for the items. 
%*  Block. The sum of the absolute differences between the values of the item. Also known as Manhattan distance. 
*  Minkowski. The $p-$th root of the sum of the absolute differences to the $p-$th power between the values for the items. 
\begin{figure}[h!]
\centering
\includegraphics[width=0.35\linewidth]{images/minkowski}
\end{figure}\\
\textit{(Remark : this is a generalization of the Euclidean distance. This one is worth remembering).}
*  Customized. The $r-$th root of the sum of the absolute differences to the $p-$th power between the values for the items.\\
\textit{(Remark: this can be seen as a generalization of the Minkowski distance, where the powers are not required to be equal).}


